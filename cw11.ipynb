{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classwork 11\n",
    "## Sakthi Kasthurirengan & Will Smith\n",
    "### 19 November 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following integration methods, discuss together on the white board to understand how they work for solving a first-order ordinary differential equation (ODE) $$u'(t) = f[t, u(t)]$$ on a discretized grid $(t_0, t_1, t_2, ..., t_N)$ with points $u_k = u(t_k)$ and grid spacing $\\Delta t = t_{k+1} - t_k$.\n",
    "<br\\>\n",
    "<br\\>\n",
    "The quality that each of these integration methods have in common is that they are all recursively defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euler's Method:**\n",
    "\n",
    "$$u_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$$\n",
    "\n",
    "To be able to use Euler's Method you need to be given the very first point of your function. This is usually done in differential equations as the initial condition. The initial condition can be thought of as the first element of $u$ or $u_0 = u(t_0)$ which will then also give you $f_0 = f(t_0,u_0)$. Then by knowing the initial and final $t$ and selecting a number of integration points you can determine your $\\Delta t$. You now have all of the pieces to get the very next point, in this case $u_1 = u(t_1)$. Then $u_1$ will be used again in this prescription to get $u_2$ with the very same $\\Delta t$ and so on. <br\\>\n",
    "<br\\>\n",
    "Since $f[t_k, u_k]$ is pretty much the derivative of the function which is described by the differential equation, what this method ends up looking like is taking your current function point and the current function slope and moving that current point in the direction of the current slope scaled by a value of $\\Delta t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leapfrog (Midpoint) Method:** \n",
    "   \n",
    "   $$u_{k+1} = u_{k-1} + 2\\Delta t\\, f[t_k, u_k]$$\n",
    "\n",
    "Leapfrog Method is very similar to Euler's method. The difference is that instead of needing only one initial point for this method, you need two. Or if you are only given one, you can use Euler's method to get the approximate second point and then use Leapfrog method to finish the rest of the approximation. <br\\>\n",
    "\n",
    "The way this works is, to get the next approximated point, the derivative at the current point multiplied by a scaling facter of $2\\Delta t$ is added to the previously approximated point, thus getting you the next approximated point. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Heun's (Trapezoid) Method:** \n",
    "   \n",
    "   $$\\tilde{u}_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$$\n",
    "   \n",
    "   $$u_{k+1} = u_k + (\\Delta t/2)(f[t_k, u_k] + f[t_{k+1}, \\tilde{u}_{k+1}])$$\n",
    "   \n",
    "If you notice, for the $\\tilde{u}_{k+1}$ terms is exactly the same as Euler's method. To get a good approximation of the next point you use Euler's method. To get an even better approximation of the next point, you add the derivative of the current point and the derivative of the next approximated point and scale it down by $\\Delta t/2$ and add that value to the current point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd-order Runge-Kutta Method:** \n",
    "   \n",
    "   $$u_{k+1} = u_k + K_2$$ \n",
    "   \n",
    "   $$K_1 = \\Delta t\\, f[t_k, u_k]$$ \n",
    "   \n",
    "   $$K_2 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_1/2]$$  \n",
    "   \n",
    "The $K_1$ term looks exactly like the additional term used in Euler's method, which would make $u_k + K_1$ be exactly Euler's method. What this method is doing is a finer approximation than Euler's. To get the next approximated point, you take your current approximated point and add it to the value of this second order approximation $K_2$.<br\\>\n",
    "\n",
    "$K_2$ is again a derivative at a certain point scaled by $\\Delta t$ (the derivative being the function in the differential equation). But the point the derivative is taken at is a bit more special than the previous methods. The derivative is taken at a $t$ value that is shifted from the current value by a factor of $\\Delta t/2$ and at the Euler approximated next point. This value multiplied by $\\Delta t$ then gives us a better approximation of the next point than if you were just to use Euler's method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**4th-order Runge-Kutta Method:** \n",
    "   \n",
    "   $$u_{k+1} = u_k + (K_1 + 2K_2 + 2K_3 + K_4)/6$$ \n",
    "   \n",
    "   $$K_1 = \\Delta t\\,f[t_k,u_k]$$\n",
    "   \n",
    "   $$K_2 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_1/2]$$ \n",
    "\n",
    "   $$K_3 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_2/2]$$ \n",
    "   \n",
    "   $$K_4 = \\Delta t\\,f[t_k + \\Delta t, u_k + K_3]$$  \n",
    " \n",
    " To use the 4-th order Runge-Kutta approximation, you need only one initial point, which gives it a large advantage to begin with. More importantly, the Runge-Kutta method creates, in a way, four approximations and then averages them in a logical way. It creates $K_1,K_2,K_3,K_4$ and weights them accordingly. \n",
    " \n",
    " $K_1$ is nothing but the change in the function, as found in Euler's approximation. It is fairly close to the point in question, but off by a little. $K_2$, then, acts as a correcting term to this. It uses the Euler approximated point and the initial point to essentially implement a Midpoint approximation between the two, but halving the timestep instead of taking two timesteps. This correction is closer to the correct value. $K_3$ is the same idea as $K_2$, but uses $K_2$ as the second point in the midpoint approximation. \n",
    " \n",
    " Since $K_2$ is close to correct, and $K_3$ gets closer to correct, one might wonder why even bother with $K_4$. The answer is that $K_4$ will underapproximate when $K_1$ overapproximates, and vice versa. The reason for this is that it uses the slope at $t+dt$. In fact, if you just took $K_1$ and $K_4$, you would be using Heun's method, averaging the slope at the initial point and the slope at the final point. However, by including $K_2$ and $K_3$, you utilize Heun's method, but with corrections via the midpoint approximation. The result is that you surround the correct value with a value far too high(relatively), a value far too low, and two values fairly close to correct, weight the two close values more than the far ones, and then take the average. If your step size is fairly small, this will give an extremely good approximation.\n",
    " \n",
    " The advantages of the Runge-Kutta 4-th Order are that you only need one initial point, and that with only four steps, you get an excellent approximation. As mentioned, one could simply continue repeating the midpoint approximation for an arbitrary number of steps, and the values would converge to the correct value, but it would take much more than four steps to get as close as Runge-Kutta 4-th Order. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
